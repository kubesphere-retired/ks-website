webpackJsonp([0x6f81deff8f32],{387:function(e,n){e.exports={data:{allMarkdownRemark:{edges:[{node:{id:"/Users/leoliu/Documents/KubeSphere/kubesphere.github.io/content/install/zh-CN/all-in-one.md absPath of file >>> MarkdownRemark",fields:{slug:"/install/zh-CN/all-in-one/",framework:"install",language:"zh-CN",article:"all-in-one"},html:'<h2>All-in-One 模式</h2>\n<p><code>All-in-One</code> 模式即单节点部署，仅建议您用来测试或熟悉部署流程和了解 KubeSphere 功能特性，在正式使用环境建议使用 <code>multi-node</code> 模式，请参考下文的 <code>multi-node</code> 模式 。</p>\n<h3>第一步: 准备节点</h3>\n<p>您可以参考以下节点规格准备一台符合要求的主机节点开始 <code>all-in-one</code> 模式的部署。</p>\n<table>\n<thead>\n<tr>\n<th>操作系统</th>\n<th>最小配置</th>\n<th>推荐配置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ubuntu 16.04 LTS 64bit</td>\n<td>CPU：8 核 \n<br/>\n 内存：12G \n<br/>\n 磁盘：40G</td>\n<td>CPU：16 核 \n<br/>\n 内存：32G \n<br/>\n 磁盘：100G</td>\n</tr>\n</tbody>\n</table>\n<h3>第二步: 准备 KubeSphere 安装文件</h3>\n<p><strong>1.</strong>  KubeSphere 官方下载地址：</p>\n<p><a href="https://drive.yunify.com/s/jV8QSnO8KkWLu4V">KubeSphere Installer 下载</a>（待更新）</p>\n<blockquote>\n<p>注：如果您无法连接外网，可参考<a href="#%E9%99%84%E5%BD%952">附录2 • 离线部署说明</a>进行离线部署。</p>\n</blockquote>\n<p><strong>2.</strong> 获取 KubeSphere 安装包后，执行以下命令解压安装包：</p>\n<pre><code class="language-shell">$ tar -zxvf kubesphere-all-express-1.0.0-alpha.tar.gz\n</code></pre>\n<p><strong>3.</strong> 进入 “<code>kubesphere-all-express-1.0.0-alpha</code>” 文件夹</p>\n<pre><code>$ cd kubesphere-all-express-1.0.0-alpha\n</code></pre>\n<h3>第三步: 执行部署</h3>\n<blockquote>\n<p>提示：</p>\n<ul>\n<li>通常情况您不需要修改任何配置，直接安装即可。</li>\n<li>若您需要自定义配置文件的安装参数，如网络、存储等相关内容需在 <strong><code>conf/vars.yml</code></strong> 配置文件中指定或修改。</li>\n<li>网络：默认插件 <code>calico</code>。</li>\n<li>支持存储类型：<code>GlusterFS、CephRBD、local-storage</code>，存储配置相关的详细信息请参考<a href="#%E9%99%84%E5%BD%953">附录3 • 存储配置说明</a>。</li>\n<li>All-in-One 默认会用 local storage 作为存储类型，由于 local storage 不支持动态分配，用户安装完毕在 KubeSphere 控制台创建存储卷的时候需要预先创建 persistent volume (PV)。Installer 预先创建了 8 个 10G local storage 的 PV 供用户直接试用。</li>\n</ul>\n</blockquote>\n<p>KubeSphere 部署过程中将会自动化地进行环境和文件监测、平台依赖软件的安装、Kubernetes 和 etcd 的自动化部署，以及存储的自动化配置。KubeSphere 安装包将会自动安装一些依赖软件，如 ansible (v2.4+)，Python-netaddr (v0.7.18+)，Jinja (v2.9+)。</p>\n<blockquote>\n<p>注：以上依赖软件的自动安装将会用到 pip 源，如果使用国内网络环境，pip 下载速度可能受限，可参考<a href="#%E9%99%84%E5%BD%951">附录1 • pip 源配置说明</a>配置国内 pip 源。</p>\n</blockquote>\n<p>当前节点的系统为 <strong><code>Ubuntu 16.04</code></strong> ，以下步骤均以 <strong><code>ubuntu</code></strong> 用户进行操作。</p>\n<p><strong>1.</strong> 进入 <code>scripts</code> 目录</p>\n<pre><code>$ cd scripts\n</code></pre>\n<p><strong>2.</strong> 执行 <code>install.sh</code> 脚本：</p>\n<pre><code>$ ./install.sh\n</code></pre>\n<p><strong>3.</strong> 输入数字 <code>1</code> 选择第一种 all-in-one 模式开始部署：</p>\n<pre><code>##################################################\nKubeSphere Installer Menu\n##################################################\n*  1）All-in-one\n*  2）Multi-node\n*  3）Quit\n##################################################\nHttps://kubesphere.io/                  2018-07-27\n##################################################\nPlease Select An Option: \n</code></pre>\n<p><strong>4.</strong> 测试 KubeSphere 单节点部署是否成功：</p>\n<p><strong>(1)</strong> 待 install.sh 执行完后，当看到如下 <code>"Successful"</code> 界面，则说明 KubeSphere 安装成功：</p>\n<pre><code>Play Rep  ****************************************\nKubeSphere   : ok=69 changed=68 unreachable=0 \nfailed=0\nSuccesful!\n##################################################\nKubeSphere is running！\nMatser IP: 121.10.121.111\nKs-console-nodeport: 32117\nKs-apiserver-nodeport 32002\n##################################################\n</code></pre>\n<p><strong>(2)</strong> 那么您就可以通过浏览器，使用集群中任一节点的 IP 地址和端口号（端口号将显示在脚本执行完之后的界面 "ks-console-nodeport" 处），也可以通过公网 IP 及端口转发的方式访问控制台，如：<a href="http://139.198.121.143:8080">http://139.198.121.143:8080</a>, 即可进入 KubeSphere 登录界面，能看到如下用户界面说明 KubeSphere 能够正常访问和使用：</p>\n<p><img src="/pic02.png"></p>\n<p>KubeSphere 部署成功后，请参考<a href="https://kubesphere.qingcloud.com">《KubeSphere 用户指南》</a>。</p>',frontmatter:{title:""}}},{node:{id:"/Users/leoliu/Documents/KubeSphere/kubesphere.github.io/content/install/zh-CN/multi-node.md absPath of file >>> MarkdownRemark",fields:{slug:"/install/zh-CN/multi-node/",framework:"install",language:"zh-CN",article:"multi-node"},html:'<h2>Multi-Node模式</h2>\n<p><code>Multi-Node</code> 即多节点集群部署，部署前建议您选择集群中任意一个节点作为一台任务执行机 <code>(taskbox)</code>，为准备部署的集群中其他节点执行部署的任务，且 taskbox 应能够与待部署的其他节点进行 <code>ssh 通信</code>。</p>\n<h3>第一步: 准备主机</h3>\n<p>您可以参考以下节点规格 准备 <strong><code>至少 2 台</code></strong> 符合要求的主机节点开始 <code>multi-node</code> 模式的部署。</p>\n<table>\n<thead>\n<tr>\n<th>操作系统</th>\n<th>最小配置</th>\n<th>推荐配置</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ubuntu 16.04 LTS 64bit</td>\n<td>CPU：8 核 \n<br/>\n 内存：12G \n<br/>\n 磁盘：40G</td>\n<td>CPU：16 核 \n<br/>\n 内存：32G \n<br/>\n 磁盘：100G</td>\n</tr>\n</tbody>\n</table>\n<p>此示例准备了 3 台主机，假设主机信息为 192.168.0.10 (node1) / 192.168.0.20 (node2) / 192.168.0.30 (node3)，以 node1 作为任务执行机 taskbox，各节点主机名可由用户自定义。</p>\n<p><strong>集群架构：</strong> 单master 单 etcd 多 node</p>\n<p><img src="/pic04.png"></p>\n<blockquote>\n<p><code>etcd</code> 作为一个高可用键值存储系统, etcd 节点个数至少需要 1 个，但部署多个 etcd 能够使集群更可靠，etcd 节点个数建议设置为<code>奇数个</code>，在当前 KubeSphere Express 版本暂支持单个 etcd 节点，将会在下一个 Advanced Edition 版本中支持 etcd 多节点部署。</p>\n</blockquote>\n<h3>第二步: 准备 KubeSphere 安装包</h3>\n<p><strong>1.</strong> KubeSphere 官方下载地址：</p>\n<p><a href="https://drive.yunify.com/s/jV8QSnO8KkWLu4V">KubeSphere Installer 下载</a>（待更新）</p>\n<blockquote>\n<p>注：如果您无法连接互联网，可参考<a href="#%E9%99%84%E5%BD%952">附录2 • 离线部署说明</a>进行离线部署</p>\n</blockquote>\n<p><strong>2.</strong> 获取 KubeSphere 安装包后，执行以下命令解压安装包：</p>\n<pre><code>$ tar -zxvf kubesphere-all-express-1.0.0-alpha.tar.gz\n</code></pre>\n<p><strong>3.</strong> 进入 “<code>kubesphere-all-express-1.0.0-alpha</code>” 文件夹</p>\n<pre><code>$ cd kubesphere-all-express-1.0.0-alpha\n</code></pre>\n<p><strong>4.</strong> 编辑主机配置文件 <code>conf/hosts.ini</code>，为了对待部署目标机器及部署流程进行集中化管理配置，集群中各个节点在主机配置文件 <code>hosts.ini</code> 中应参考如下配置：</p>\n<blockquote>\n<p>注：本示例集群一共三个节点, 各节点主机名可由用户自定义, 修改配置文件脚本时不能手动换行</p>\n</blockquote>\n<pre><code>[all]\nnode1  ansible_connection=local local_release_dir={{ansible_env.HOME}}/releases ansible_user=ubuntu ansible_become=yes ansible_become_user=root ansible_become_pass=password\nnode2  ansible_host=192.168.0.20 ip=192.168.0.20 ansible_user=ubuntu ansible_become=yes ansible_become_user=root ansible_become_pass=password\nnode3  ansible_host=192.168.0.30 ip=192.168.0.30 ansible_user=ubuntu ansible_become=yes ansible_become_user=root ansible_become_pass=password\n\n[kube-master]\nnode1\n\n[kube-node]\nnode1\nnode2\nnode3\n\n[etcd]\nnode1\n\n[k8s-cluster:children]\nkube-node\nkube-master\n</code></pre>\n<blockquote>\n<p>说明：</p>\n<ul>\n<li>[all] 中应填写集群中各个节点的内网 IP 信息及参数，node1 作为 taskbox 在 [all] 中参数仅需要将 ansible<em>become</em>pass 替换为当前任务执行机的登录密码，[all] 中其它参数比如 node2 和 node3 仅需要替换 "ansible<em>host" 和 "ip" 为当前 node2 和 node3 的内网 IP，node2 和 node3 的 "ansible</em>become_pass" 即替换为各自主机的登录密码</li>\n<li>node1 作为 taskbox，用来执行整个集群的部署任务，同时也是 kubernetes 集群的 master 节点和 etcd 节点，应填入 [kube-master] 和 [etcd] 部分</li>\n<li>node1，node2，node3 是 kubernetes 集群的 node 节点，应填入 [kube-node] 部分</li>\n</ul>\n</blockquote>\n<p><strong>5.</strong> Multi-Node 模式进行多节点部署时，您需要预先准备好对应的存储服务器，再参考<a href="#%E9%99%84%E5%BD%953">附录3 • 存储配置说明</a> 配置集群的存储类型。网络、存储等相关内容需在 <code>vars.yml</code> 配置文件中指定或修改，可执行以下命令通过Vim对 <code>vars.yml</code> 进行编辑：</p>\n<pre><code>$ vi conf/vars.yml\n</code></pre>\n<blockquote>\n<p>说明：</p>\n<ul>\n<li>根据配置文件按需修改相关配置项，未做修改将以默认参数执行。</li>\n<li>网络：默认插件<code>calico</code></li>\n<li>支持存储类型：<code>GlusterFS、CephRBD</code>， 存储配置相关的详细信息请参考<a href="#%E9%99%84%E5%BD%953">附录3 • 存储配置说明</a></li>\n</ul>\n</blockquote>\n<h3>第三步: 安装 KubeSphere</h3>\n<p>KubeSphere 多节点部署同样会自动化地进行环境和文件监测、平台依赖软件的安装、<code>Kubernetes</code> 和 <code>etcd</code> 集群的自动化部署，以及存储的自动化配置。KubeSphere 安装包将会自动安装一些依赖软件，如 ansible (v2.4+)，Python-netaddr (v0.7.18+)，Jinja (v2.9+)。</p>\n<blockquote>\n<p>注：安装以上依赖软件需要用到 pip 源，如果使用国内网络环境，pip 下载速度可能受限，可参考<a href="#%E9%99%84%E5%BD%951">附录1 • pip 源配置说明</a>配置国内 pip 源</p>\n</blockquote>\n<p>当前节点的系统为 <strong><code>Ubuntu 16.04</code></strong> ，以下步骤均以 <strong><code>ubuntu</code></strong>  用户进行操作。</p>\n<p>请按以下步骤执行 KubeSphere 多节点部署：</p>\n<p><strong>1.</strong> 进入 <code>scripts</code> 目录</p>\n<pre><code>$ cd scripts\n</code></pre>\n<p><strong>2.</strong> 执行 <code>install.sh</code> 脚本：</p>\n<pre><code>$ ./install.sh\n</code></pre>\n<p><strong>3.</strong> 输入数字 <code>2</code> 选择第二种 multi-node 模式开始部署：</p>\n<pre><code>##################################################\nKubeSphere Installer Menu\n##################################################\n*  1）All-in-one\n*  2）Multi-node\n*  3）Quit\n##################################################\nHttps://kubesphere.io/                  2018-07-27\n##################################################\nPlease Select An Option: 2\n2\n</code></pre>\n<p><strong>4.</strong> 测试 KubeSphere 集群部署是否成功：</p>\n<p><strong>(1)</strong> 待 <code>install.sh</code> 执行完后，当看到如下 "Successful" 界面，则说明 KubeSphere 安装成功：</p>\n<pre><code>Play Rep  ****************************************\nKubeSphere   : ok=69 changed=68 unreachable=0 \nfailed=0\nSuccesful!\n##################################################\nKubeSphere is running！\nMatser IP: 10.160.6.6\nKs-console-nodeport: 32117\nKs-apiserver-nodeport 32002\n##################################################\n</code></pre>\n<p><strong>(2)</strong> 那么您就可以通过浏览器，使用集群中任一节点的 IP 地址和端口号（端口号将显示在脚本执行完之后的界面 "ks-console-nodeport" 处），也可以通过公网 IP 及端口转发的方式访问控制台，如：<a href="http://139.198.121.143:8080">http://139.198.121.143:8080</a>, 即可进入 KubeSphere 登录界面，能看到如下用户界面说明 KubeSphere 能够正常访问和使用：</p>\n<p><img src="/pic02.png"></p>\n<p>KubeSphere 部署成功后，请参考<a href="https://kubesphere.qingcloud.com">《KubeSphere 用户指南》</a>。</p>\n<hr>\n<h2>附录1：pip 源配置说明</h2>\n<blockquote>\n<p>如果使用国内网络环境，pip 下载速度可能受限，可按如下方法配置国内 pip 源</p>\n</blockquote>\n<pre><code>$ mkdir ~/.pip\n</code></pre>\n<pre><code>$ cat > ~/.pip/pip.conf &#x3C;&#x3C; EOF\n[global]\ntrusted-host=mirrors.aliyun.com\nindex-url=https://mirrors.aliyun.com/pypi/simple/\nEOF\n</code></pre>\n<h2>附录2：离线部署说明</h2>\n<ol>\n<li>下载安装包以及离线部署 Repos</li>\n<li>解压安装包，并将 Repos 解压后拷贝到安装包中的 Repos 目录下</li>\n<li>执行 installer 初始化脚本: scripts/InitInstaller.sh，安装 installer 相关依赖软件。</li>\n<li>修改配置文件，同在线安装。</li>\n<li>执行部署</li>\n</ol>\n<pre><code>$ scripts/install.sh\n</code></pre>\n<h2>附录3：存储配置说明</h2>\n<h3>配置存储类型</h3>\n<p>可使用 <code>GlusterFS</code>、<code>CephRBD</code> 作为持久化存储，需提前准备相关存储服务端。</p>\n<p> 在您准备好存储服务端以后，只需要参考以下表中的参数说明，在 <code>conf</code> 目录下的 <code>vars.yml</code> 中，根据您存储服务端所支持的存储类型，在 <code>vars.yml</code> 的 <code># Ceph_rbd deployment</code> 或 <code># GlusterFS provisioner deployment</code> 或 <code># Local volume provisioner deployment(Only all-in-one)</code> 部分，参考脚本中的示例修改对应参数，即可完成 Kubernetes 集群存储类型的配置。</p>\n<blockquote>\n<ol>\n<li>KubeSphere 安装过程中程序将会根据用户在 vars.yml 里选择配置的存储类型如 GlusterFS 或 CephRBD，进行自动化地安装对应 Kubernetes 集群所需的GlusterFS Client 或 CephRBD Client，无需手动安装 Client。</li>\n<li><code>Ceph</code> 集群部署可参考 <a href="http://docs.ceph.com/docs/master/">Install Ceph</a></li>\n<li><code>Gluster</code> 集群部署可参考 <a href="https://www.gluster.org/install/">Install Gluster</a> 或 <a href="http://gluster.readthedocs.io/en/latest/Install-Guide/Install/">Gluster Docs</a> 并且需要安装<a href="https://github.com/heketi/heketi/tree/master/docs/admin">Heketi 管理端</a></li>\n<li>Kubernetes 集群中不可同时存在两个默认存储类型，若要指定默认存储类型前请先确保当前集群中无默认存储类型。</li>\n</ol>\n</blockquote>\n<p>以下对存储相关配置做简要说明(参数详解请参考 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">storage classes</a> )：</p>\n<table>\n<thead>\n<tr>\n<th><strong>Local volume</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>local\n_\nvolume\n_\nprovisioner\n_\nenabled</td>\n<td>是否使用local\n_\nvolume作为持久化存储  是: true; 否: false</td>\n</tr>\n<tr>\n<td>local\n_\nvolume\n_\nprovisioner\n_\nstorage\n_\nclass</td>\n<td>storage\n_\nclass名称   默认：local-storage</td>\n</tr>\n<tr>\n<td>local\n_\nvolume\n_\nprovisioner\n_\nnamespace</td>\n<td>local\n_\nvolume相关资源对象所在namespace  默认：kube-system</td>\n</tr>\n<tr>\n<td>local\n_\nvolume\n_\nis\n_\ndefault\n_\nclass</td>\n<td>是否设定为默认storage\n_\nclass 是: true; 否: false \n<br/>\n 注：系统中存在多种storage\n_\nclass时，只能设定一种为：default\n_\nclass</td>\n</tr>\n<tr>\n<td>local\n_\nvolume\n_\nprovisioner\n_\nbase\n_\ndir</td>\n<td>主机存储盘挂载路径  默认：/mnt/disks</td>\n</tr>\n</tbody>\n</table>\n<br/>\n<table>\n<thead>\n<tr>\n<th><strong>Ceph_RBD</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ceph\n_\nrbd\n_\nenabled</td>\n<td>是否使用 ceph\n_\nRBD 作为持久化存储，是: true; 否: false</td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nstorage\n_\nclass</td>\n<td>storage\n_\nclass 名称</td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nis\n_\ndefault\n_\nclass</td>\n<td>是否设定为默认 storage\n_\nclass 是: true; 否: false \n<br/>\n 注：系统中存在多种 storage\n_\nclass 时，只能设定一种为：default\n_\nclass</td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nmonitors</td>\n<td>根据 Ceph RBD 服务端配置填写，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nadmin\n_\nid</td>\n<td>能够在存储池中创建 images 的客户端 ID 默认: admin，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nadmin\n_\nsecret</td>\n<td>Admin_id 的 secret，安装程序将会自动在 kube-system 项目内创建此 secret，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\npool</td>\n<td>可使用的 CephRBD 存储池，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nuser\n_\nid</td>\n<td>用于映射 RBD images 的 ceph 客户端 ID 默认: admin，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nuser\n_\nsecret</td>\n<td>User_id 的 secret，需注意在所使用 rbd image 的项目内都需创建此 Secret，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nfsType</td>\n<td>kubernetes 支持的 fsType，默认：ext4，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nimageFormat</td>\n<td>CephRBD images 格式，默认："1"，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>ceph\n_\nrbd\n_\nimageFeatures</td>\n<td>当 ceph\n<em>rbd</em>\nimageFormat 字段不为 1 时需填写此字段，可参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Kubernetes 官方文档</a></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>注： 存储类型中创建 secret 所需 ceph secret 如 ceph<em>rbd</em>admin<em>secret 和 ceph</em>rbd<em>user</em>secret 可在 ceph 服务端通过以下命令获得：</p>\n</blockquote>\n<pre><code>$ ceph auth get-key client.admin\n</code></pre>\n<br/>\n<table>\n<thead>\n<tr>\n<th><strong>GlusterFS（需提供 heketi 所管理的 glusterfs 集群）</strong></th>\n<th><strong>Description</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nenabled</td>\n<td>是否使用 GlusterFS 作为持久化存储，是: true; 否: false</td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nstorage\n_\nclass</td>\n<td>storage\n_\nclass 名称</td>\n</tr>\n<tr>\n<td>glusterfs\n_\nis\n_\ndefault\n_\nclass</td>\n<td>是否设定为默认 storage\n_\nclass，是: true; 否: false \n<br/>\n 注：系统中存在多种 storage\n_\nclass 时，只能设定一种为：default\n_\nclass</td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nrestauthenabled</td>\n<td>Gluster 启用对 REST 服务器的认证,参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nresturl</td>\n<td>Heketi 服务端 url，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nclusterid</td>\n<td>Gluster 集群 id，登录 heketi 服务端输入 heketi-cli cluster list 得到 Gluster 集群 id，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nrestuser</td>\n<td>能够在 Gluster pool 中创建 volume 的 Heketi 用户，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nsecretName</td>\n<td>secret 名称，安装程序将会在 kube-system 项目内自动创建此 secret，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\ngidMin</td>\n<td>glusterfs\n_\nprovisioner\n_\nstorage\n_\nclass 中 GID 的最小值，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\ngidMax</td>\n<td>glusterfs\n_\nprovisioner\n_\nstorage\n_\nclass 中 GID 的最大值，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>glusterfs\n_\nprovisioner\n_\nvolumetype</td>\n<td>Volume 类型，参数配置请参考\n<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#glusterfs">Kubernetes 官方文档</a></td>\n</tr>\n<tr>\n<td>jwt\n_\nadmin\n_\nkey</td>\n<td>heketi 服务器中 /etc/heketi/heketi.json 的 jwt.admin.key 字段</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>注： Glusterfs 存储类型中所需的 <code>glusterfs_provisioner_clusterid</code> 可在 glusterfs 服务端通过以下命令获得：</p>\n</blockquote>\n<pre><code> $ export HEKETI_CLI_SERVER=http://localhost:8080\n $ heketi-cli cluster list\n</code></pre>\n<h3>local volume 使用方法(不属于 KubeSphere 安装步骤)</h3>\n<p>此小节不属于 KubeSphere 安装步骤，仅帮助用户使用 local volume。</p>\n<blockquote>\n<p>local volume 数据卷 <strong>仅用于 all-in-one 单节点部署</strong>。</p>\n</blockquote>\n<p>使用 <code>local volume</code> 的基本流程需要参考如下步骤：</p>\n<ol>\n<li>\n<p>预先在宿主机创建文件夹</p>\n</li>\n<li>\n<p>创建 PV</p>\n</li>\n<li>\n<p>通过 KubeSphere 创建数据卷</p>\n</li>\n</ol>\n<p>具体步骤如下：</p>\n<ul>\n<li>登录宿主机，创建文件夹，以文件夹 vol-test 为例，执行以下命令：</li>\n</ul>\n<pre><code>$ mkdir -p /mnt/disks/vol-test\n</code></pre>\n<ul>\n<li>\n<p>创建 pv</p>\n<ul>\n<li>pv.yaml 文件定义</li>\n</ul>\n</li>\n</ul>\n<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-local\nspec:\n  capacity:\n    storage: 10Gi \n  # volumeMode field requires BlockVolume Alpha feature gate to be enabled.\n  volumeMode: Filesystem\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local\n  local:\n    path: /mnt/disks/vol-test\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - kubesphere\n</code></pre>\n<ul>\n<li>执行创建命令</li>\n</ul>\n<pre><code>$ kubectl create -f pv-local.yaml\n</code></pre>\n<ul>\n<li>执行以下命令验证创建结果</li>\n</ul>\n<pre><code>$ kubectl get pv\nNAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM        STORAGECLASS    REASON    AGE\npv-local     10Gi       RWO            Delete           Available                local                     4s\n</code></pre>\n<ul>\n<li>上述工作完成后可在 KubeSphere UI 创建存储卷：</li>\n</ul>\n<blockquote>\n<p>注：Local volume 存储卷创建成功后为 Pending 属于正常状态，当创建 Workload 调度 Pod 后存储卷状态即可变化为 Bound</p>\n</blockquote>\n<ul>\n<li>\n<p>如何清理存储卷</p>\n<ol>\n<li>\n<p>KubeSphere UI 删除当前未挂载的存储卷</p>\n</li>\n<li>\n<p>登录宿主机，删除 PV</p>\n</li>\n<li>\n<p>删除 PV 关联的文件夹以清理数据</p>\n</li>\n</ol>\n</li>\n</ul>\n<blockquote>\n<p>Local Volume 不支持动态分配 (Dynamic Provisioning) 方式， 如果希望体验 KubeSphere 推荐的动态分配 (Dynamic Provisioning) 方式创建存储卷，请配置 <code>GlusterFS</code> 和 <code>Ceph RBD</code> 存储服务端参数。</p>\n</blockquote>',frontmatter:{title:""}}}]}},pathContext:{framework:"install",language:"zh-CN"}}}});